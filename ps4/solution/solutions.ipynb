{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Backprop in a simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1**: $\\nabla_{a^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $g \\leftarrow \\nabla_{a^2} J = g \\odot f'(a^2) = g \\odot h^2(1-h^2) = h^2 - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2**: $\\nabla_{b^2}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln:** for $k=2$, $\\nabla_{b^2} J = g  = h^2 - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3**: $\\nabla_{W^2}J$ <br><i>Hint: this should be a vector, since $W^2$ is a vector. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $\\nabla_{W^2}J = g (h^1)^T = (h^2 - y)(h^1)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4**: $\\nabla_{h^1}J$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** for $k=2$, $\\nabla_{h^1}J = (W^2)^T g = (h^2 - y)(W^2)^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.5**: $\\nabla_{b^1}J$, $\\nabla_{W^1}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Soln:** $\\nabla_{b^1}J= (h^{2} - y)((h^{1})^2 - h^{1}) \\odot W^{2}$, $\\nabla_{W^1}J = (h^{2} - y) (h^{0}) [((h^{1})^2 - h^{1}) \\odot W^{2}]  ^ T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.6** Briefly, explain how would the computational speed of backpropagation be affected if it did not include a forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln** It would increase significantly, as we'd need to re-compute activations h many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (Programming): Implementing a simple MLP\n",
    "\n",
    "See example implementation in `solutions_mlp.py`\n",
    "\n",
    "When you compare the networks using different activations, you should see that Sigmoid underperforms (~86% test accuracy), and that ReLU have higher performance (~96% test accuracy). ReLU is easy to code, and does not have saturation problems and numerical issues, therefore it is preferred in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Simple Regularization Methods\n",
    "\n",
    "**Q3.1**:  L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln**: $\\theta_{t+1}\\gets (1-2\\lambda\\eta)\\theta_{t}-\\eta g_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2**:  L1 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soln**: $\\theta_{t+1}\\gets \\theta_t-\\eta\\lambda \\text{sign}(\\theta_t)- \\eta g_t$, where $\\text{sign}(\\theta_{t,i})=\\left\\{\\begin{matrix} 1 & \\theta_{t,i}\\geq 0 \\\\ -1 & \\theta_{t,i}<0 \\end{matrix}\\right.$.\n",
    "\n",
    "Note: it is also acceptable if you define $\\text{sign}(0)=-1$ or $0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
